{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5c51926-607d-42f5-adca-9acac7032e3f",
   "metadata": {},
   "source": [
    "## Basic Query\n",
    "Let's practice performing a general query using Python and ChatGPT API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba70941-17b1-4c74-9300-56591b8b6ebb",
   "metadata": {},
   "source": [
    "Here is the basic syntax. Please note: This may not work idefinitely. Syntax changes! And if you find old syntax in documentation on the internet from a few weeks or months ago, you have to use the new.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c053561-b8e6-4579-857f-bda3bb4adcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "#You must set your environ:\n",
    "os.environ['OPEN_AI_KEY'] = \"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\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15126128-fd1c-4141-91cf-8387f7e1274f",
   "metadata": {},
   "source": [
    "In your own words, explain to me what os.environ is, and why we should use it when accessing an api with a key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a31249-cb53-4bff-8861-2669a0dce133",
   "metadata": {},
   "source": [
    "**1. When I say \"your own words\" please don't copy and paste your answer from ChatGPT. Your answer here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42c7458-2fe9-4e25-a82d-068dd1059991",
   "metadata": {},
   "source": [
    "Now that your api key is set, we neex to use the OpenAI() function to \"get\" the api key and set it equal to a variable. The convention is to set it to \"api_key\" The OpenAI function takes the environ.get() function as an argument. See below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49602809-2e4c-4f8b-b164-8e8430fe855c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"winner\": \"France\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPEN_AI_KEY\"),\n",
    ")\n",
    "\n",
    "# Now we use this syntax to get a response, format it, and assign roles.\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-0125\",\n",
    "  response_format={ \"type\": \"json_object\" },\n",
    "  messages=[\n",
    "    {\"role\": \"system\",\n",
    "     \"content\": \"You are a helpful assistant who answers all of my queries in json.\"},\n",
    "    {\"role\": \"user\", \n",
    "     \"content\": \"Who won the world cup in 2018?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05071d93-d74a-48fd-9979-0a9b51ac45b2",
   "metadata": {},
   "source": [
    "The code above contains some format and system content instructions to force the model to return json. What if you were to remove the response format instruction. Would it still work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2224fef2-c4d1-4b96-8ca0-87e25928b0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"winner\": \"France\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Your code and response here: Yes, it will still work\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPEN_AI_KEY\"),\n",
    ")\n",
    "\n",
    "# Now we use this syntax to get a response, format it, and assign roles.\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-0125\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\",\n",
    "     \"content\": \"You are a helpful assistant who answers all of my queries in json.\"},\n",
    "    {\"role\": \"user\", \n",
    "     \"content\": \"Who won the world cup in 2018?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec9bb6e",
   "metadata": {},
   "source": [
    "3. Can you remove the response_format instructions and prompt it to a text response?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3faa4de-bb01-4da6-b9cf-3f9a90fb7afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2018 FIFA World Cup was won by the French national football team. They defeated Croatia 4-2 in the final to clinch their second World Cup title.\n"
     ]
    }
   ],
   "source": [
    "#Your code and response here:\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPEN_AI_KEY\"),\n",
    ")\n",
    "\n",
    "# Now we use this syntax to get a response, format it, and assign roles.\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-0125\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\",\n",
    "     \"content\": \"You are a helpful assistant who answers all of my queries in text\"},\n",
    "    {\"role\": \"user\", \n",
    "     \"content\": \"Who won the world cup in 2018?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ed0c3c-af2e-494b-8ec8-15fe7560142f",
   "metadata": {},
   "source": [
    "4. What if you had the response_format type set to \"json_object\" but then instructed the system content to produce text? What would occur?\n",
    "What if you were to set the response_format type to \"text\" but then instructed the system content to produce a json object? What would occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a91357a1-2092-4c36-895b-911443bdc0dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# This is the default and can be omitted\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPEN_AI_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Now we use this syntax to get a response, format it, and assign roles.\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo-0125\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m  \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson_object\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant who answers all of my queries in text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWho won the world cup in 2018?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m  \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# text but produce a json_object ----------------\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#Your code and response here:\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/openai/resources/chat/completions.py:663\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    662\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:1200\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1188\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1196\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1197\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1198\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1199\u001b[0m     )\n\u001b[0;32m-> 1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:889\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    882\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    888\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:980\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    977\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    979\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    983\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    984\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    988\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}"
     ]
    }
   ],
   "source": [
    "# Your code and response here:\n",
    "\n",
    "#json_object but produce a text--------- (DOES NOT WORK)\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPEN_AI_KEY\"),\n",
    ")\n",
    "\n",
    "# Now we use this syntax to get a response, format it, and assign roles.\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-0125\",\n",
    "  response_format={ \"type\": \"json_object\" },\n",
    "  messages=[\n",
    "    {\"role\": \"system\",\n",
    "     \"content\": \"You are a helpful assistant who answers all of my queries in text\"},\n",
    "    {\"role\": \"user\", \n",
    "     \"content\": \"Who won the world cup in 2018?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0608de1-b7c8-48ff-a58f-44a99d5d4487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"winner\": \"France\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# text but produce a json_object ----------------(WORKS)\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPEN_AI_KEY\"),\n",
    ")\n",
    "\n",
    "# Now we use this syntax to get a response, format it, and assign roles.\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-0125\",\n",
    "  response_format={ \"type\": \"text\" },\n",
    "  messages=[\n",
    "    {\"role\": \"system\",\n",
    "     \"content\": \"You are a helpful assistant who answers all of my queries in json_object.\"},\n",
    "    {\"role\": \"user\", \n",
    "     \"content\": \"Who won the world cup in 2018?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c982b67",
   "metadata": {},
   "source": [
    "5. What would happen if we left out the system prompt completely? Why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6527eb66-891b-494e-8a24-ae5f3b40a9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France won the FIFA World Cup in 2018.\n"
     ]
    }
   ],
   "source": [
    "#Your code and response here: \n",
    "#Why answer- Since it's a question that has an object answer, the system does not have to assume a role to answer\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPEN_AI_KEY\"),\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-0125\"  ,\n",
    "  response_format={\"type\": \"text\"},\n",
    "  messages = [\n",
    "      {\"role\": \"user\",\n",
    "      \"content\": \"Who won the world cup in 2018?\"}\n",
    "  ]\n",
    "\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d5f621-460f-4a1b-bd34-3188126d72e0",
   "metadata": {},
   "source": [
    "6. Create a poem on the joys of studying Python with Professor King at Yale using ChatGPT. Choose a particular poetic style or poet to emulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8bd01987-435e-492e-b202-ee43c488a2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Python, noble serpent of the code,\n",
      "In ivory towers, its wonders glow.\n",
      "With Professor King at Yale, we bask,\n",
      "In knowledge deep, a challenging task.\n",
      "\n",
      "Through corridors of syntax we roam,\n",
      "In ancient libraries, make our home.\n",
      "Pizza feasts, a reward well earned,\n",
      "As wisdom's flame within us burned.\n",
      "\n",
      "Crafts our homework with skilled finesse,\n",
      "Tailored to our passions, a true success.\n",
      "Each line of code, a work of art,\n",
      "In classrooms filled with eager hearts.\n",
      "\n",
      "Oh, how bittersweet, these days now past,\n",
      "In memories, they forever last.\n",
      "For in the kingdom of Python, we found,\n",
      "A treasure trove of knowledge profound.\n",
      "\n",
      "To Professor King, a gratitude sincere,\n",
      "For guiding us, year after year.\n",
      "In the realm of code, a place so grand,\n",
      "We found our purpose, hand in hand.\n"
     ]
    }
   ],
   "source": [
    "#Your code here:\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPEN_AI_KEY\"),\n",
    ")\n",
    "\n",
    "response=client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "    response_format={\"type\":\"text\"},\n",
    "    messages=[\n",
    "        {\"role\": \"system\",\n",
    "        \"content\": \"Write all answers in a melancholic ode poetic style\"},\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": \"Create a poem on the joys of studying Python with Professor King at Yale. For more information, he buys us pizza when we meet in person and crafts our homework based the students interests.\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af17b12-be3e-49ac-8c84-10e79a73a7e6",
   "metadata": {},
   "source": [
    "## Adding specific tone, temperature and max tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a2d9b-b5e4-4827-95ce-e90988584198",
   "metadata": {},
   "source": [
    "Ask gpt-3.5-turbo to explain transformers architecture to you at a second grade, 8th grade and 11th grade level. Use 3 different temperatures for each .1, 1, and 1.7. Set Max tokens to 100. Compare the results for each. Is there a different ideal temperature setting depending on the educational level? Is there a greater requirement for precision as the grade level increases? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ab892e5-1433-44f0-bbf4-f9efb9d9be53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "FOR 2nd GRADE:\n",
      "\n",
      "Temperature = 0.1\n",
      " Alright, imagine you have two friends named Optimus and Bumblebee. They are both robots that can change their shape to become a big truck or a cool car. \n",
      "\n",
      "In the Transformers world, there are two main parts to these robots: the Autobots and the Decepticons. The Autobots are the good guys, like Optimus and Bumblebee, and the Decepticons are the bad guys.\n",
      "\n",
      "Now, think of a big book that you want to read. This\n",
      "\n",
      "Temperature = 1\n",
      " Alright! Imagine you have two friends, Optimus Prime and Bumblebee, who can transform into different things. When they are in their robot form, they can't talk to each other because they speak different languages. So, they use a special machine called a Transformer to help them understand each other.\n",
      "\n",
      "The Transformer machine has two parts - an encoder and a decoder. The encoder listens to Optimus Prime and changes what he says into a language that both of them understand. It then sends this new\n",
      "\n",
      "Temperature = 1.7\n",
      " Alright, imagine you have two friends, Optimus Prime and Bumblebee. They both know how to change from a big truck to a really cool robot. Imagine that the big truck is like an input sentence (sequence of words) and the robot is like the transformed or translated sentence (also a sequence of words).\n",
      "\n",
      "Now, the Transformers like Optimus Prime and Bumblebee have a magic power called attention – they can pay attention to other parts of themselves as they take different forms. This helps\n",
      "\n",
      "\n",
      "\n",
      "FOR 8th GRADE:\n",
      "\n",
      "Temperature = 0.1\n",
      " Sure! Imagine you have a big book that you want to summarize into a shorter version. Transformers are like a special kind of machine that helps you do this summarization task for any kind of text.\n",
      "\n",
      "In a transformer, there are two main parts called the encoder and the decoder. The encoder reads the input text and tries to understand the important parts of it. It breaks down the text into smaller pieces and learns the relationships between these pieces.\n",
      "\n",
      "The decoder then takes this understanding from the encoder and uses it\n",
      "\n",
      "Temperature = 1\n",
      " Alright, let's break it down! Imagine you have a big transformer toy with two sides - one is called the encoder and the other is called the decoder. \n",
      "\n",
      "The encoder side takes in a sentence or a bunch of words and turns them into special codes that represent the meaning of the words. It's like the encoder is translating the words into a secret language that only it understands.\n",
      "\n",
      "Then, the decoder side takes those special codes and tries to figure out what the original sentence was. It's like\n",
      "\n",
      "Temperature = 1.7\n",
      " Absolutely, I'd be happy to explain! Think of a transformer like two machines that change an input into an output. Kind of like a vending machine - you put in money (your input) and get out a snack (your output).\n",
      "\n",
      "A transformer has two main parts: the front end (encoder) and the back end (decoder). The job of the encoder is to understand and learn about the input, also known as \"self-          what you are putting into the vending machine. The decoder\n",
      "\n",
      "\n",
      "\n",
      "FOR 11th GRADE:\n",
      "\n",
      "Temperature = 0.1\n",
      " Sure! Let's break it down step by step.\n",
      "\n",
      "1. **What is a Transformer?**\n",
      "   - A Transformer is a type of neural network architecture that is commonly used in natural language processing tasks, such as language translation and text generation.\n",
      "\n",
      "2. **Components of a Transformer:**\n",
      "   - **Encoder:** This part of the Transformer processes the input text and converts it into a series of numerical representations called embeddings.\n",
      "   - **Decoder:** The Decoder takes the embeddings generated by the Encoder and uses\n",
      "\n",
      "Temperature = 1\n",
      " Absolutely! I'll break it down for you.\n",
      "\n",
      "Imagine you have a piece of text that you want to understand better. The transformer architecture is like a smart system that helps you analyze and process this text effectively.\n",
      "\n",
      "At the heart of the transformer are two key components: the encoder and the decoder. The encoder takes your input text and converts it into a form that the transformer model can work with. It breaks down the text into smaller, meaningful chunks called “tokens” and then processes these tokens in parallel\n",
      "\n",
      "Temperature = 1.7\n",
      " Of course! Let's break it down in simpler terms.\n",
      "\n",
      "Imagine a transformer model as a smart and powerful magician practicing a magic show, but instead of focusing on the whole magic trick at once, he is more concerned with pieces of the trick, one small detail at a time.\n",
      "\n",
      "For this analogy, let's imagine any paragraph you give to the magician as text, like a sentence from a book. The Transformers model or the magician receives the sentence, then divides it into smaller segments, meaning each word\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "\n",
    "for grade_number in [\"2nd\", \"8th\", \"11th\"]:\n",
    "    print(f\"\\n\\nFOR {grade_number} GRADE:\\n\")\n",
    "    for temperature_number in [.1, 1, 1.7]:\n",
    "        client = OpenAI(\n",
    "            api_key=os.environ.get(\"OPEN_AI_KEY\"),\n",
    "        )\n",
    "\n",
    "        response=client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-0125\",\n",
    "            response_format={\"type\":\"text\"},\n",
    "            messages=[\n",
    "                {\"role\": \"system\",\n",
    "                \"content\": \"You are a very helpful teacher who doesn't mind going back to the basics if it means their student will have a better understanding\"},\n",
    "                {\"role\": \"user\",\n",
    "                \"content\": f\"Explain the transformers architecture to me like I was in the {grade_number} grade.\"}\n",
    "            ],\n",
    "            temperature = temperature_number,\n",
    "            max_tokens = 100\n",
    "        )\n",
    "        print(f\"Temperature = {temperature_number}\\n {response.choices[0].message.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59bea90-bcf5-4b30-9839-97c5ebf00b45",
   "metadata": {},
   "source": [
    "## Your analysis of temperature here:\n",
    "\n",
    "For 2nd grade: tempature = 1 is the best\n",
    "\n",
    "For 8th grade: temperature = 0.1 was able to get the bigger picture, so it was the best.\n",
    "\n",
    "For 11th grade: 0.1 is good if you have some familarity with machine learning/word embeddings. 1 is good if you have no familarity and what to get the big picture down.\n",
    "\n",
    "\n",
    "We also see that as the grade level increases, there was less use of examples:\n",
    "- 2nd grade -> 3 examples\n",
    "- 8th grade -> 2 examples\n",
    "- 11th grade -> 1 example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605dbd09-27a4-4566-b75d-7791263e7b51",
   "metadata": {},
   "source": [
    "## Reading a PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd22096-ab3a-40e7-aa07-32746ed158bf",
   "metadata": {},
   "source": [
    "We can interact with pdfs using the api. First we need to get the pdf, then convert it to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78d6e41f-adb2-4e70-9d51-36e32c271264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2146k  100 2146k    0     0  4742k      0 --:--:-- --:--:-- --:--:-- 4738k\n"
     ]
    }
   ],
   "source": [
    "#Here is the code to get a pdf using the command line \"curl\" command\n",
    "\n",
    "!curl -o paper.pdf https://people.csail.mit.edu/brooks/idocs/Turing_Paper_1936.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bd1c66-9f51-4d80-bdb6-a6b8ab49d7ee",
   "metadata": {},
   "source": [
    "Now we need to install the pdf reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a5ffc9-0b2b-432d-a81a-29222b6a435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e85ad34-15fd-4b1a-a243-b128e0a9aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPEN_AI_KEY'] = '--your api key--'\n",
    "\n",
    "# api_key = os.environ['OPEN_AI_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd5cbd7-61fd-482d-b5e1-53c9871c19bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import re\n",
    "import openai\n",
    "\n",
    "import os\n",
    "\n",
    "# Instantiate a client to ChatGPT and grab the API key from the .env file\n",
    "# client = OpenAI(api_key=os.environ.get())\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPEN_AI_KEY\"),\n",
    ")\n",
    "\n",
    "# Set the string that will contain the summary\n",
    "#client = OpenAI\n",
    "pdf_summary_text = \"\"\n",
    "# Open the PDF file\n",
    "pdf_file_path = \"--this is where you put the path to your pdf that you downloaded\"\n",
    "# Read the PDF file using PyPDF2\n",
    "pdf_file = open(pdf_file_path, 'rb')\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "# Loop through all the pages in the PDF file\n",
    "for page_num in range(len(pdf_reader.pages)):\n",
    "    # Extract the text from the page\n",
    "    page_text = pdf_reader.pages[page_num].extract_text().lower()\n",
    "    print(page_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad41ed1c-55f7-49e9-b5e7-71b0f7d345f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful research assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": f\"Summarize this: {page_text}\"},\n",
    "                            ],\n",
    "                                )\n",
    "page_summary = response.choices[0].message.content\n",
    "\n",
    "\n",
    "#WE may want to write the summary to a file so that we can access it later\n",
    "\n",
    "pdf_summary_text+=page_summary + \"\\n\"\n",
    "pdf_summary_file = pdf_file_path.replace(os.path.splitext(pdf_file_path)[1], \"_summary.txt\")\n",
    "with open(pdf_summary_file, \"w+\") as file:\n",
    "    file.write(pdf_summary_text)\n",
    "\n",
    "pdf_file.close()\n",
    "\n",
    "with open(pdf_summary_file, \"r\") as file:\n",
    "    print(file.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f9411e-e414-4fa8-85cf-c69fde5c80a4",
   "metadata": {},
   "source": [
    "Now it's your turn\n",
    "1. Grab a pdf of your choosing from the internet using curl\n",
    "2. Convert the pdf to text\n",
    "3. Write a summary of the pdf in the style of Kim Kardashian, or your favorite, influencer, politician or tv personality of your choosing.\n",
    "4. *Some hints: You may have to use prompt technique \"best practices\" to make the model proceed by steps to accomplish your task. You may have to provide examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd4e71-c9ef-4283-93e0-935fa41bf231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b00f40b-2fe7-4594-b0df-e8d9e2c586ef",
   "metadata": {},
   "source": [
    "## Question and Answer Task on a Long Text\n",
    "We'd like to do some Q and A on a long text. But the problem is that the context window can't always accomodate long pdfs. We need to split the text into smaller chunks, and vectorize them (for our Q and A).\n",
    "\n",
    "The Langchain package is an opensource package to interact with LLMs. We will be using it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0accb-f78a-4ea6-a8e5-99f7ca0deef6",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Download the PDF Reid Hoffman book with GPT-4 from his free download link\n",
    "!wget -q https://www.impromptubook.com/wp-content/uploads/2023/03/impromptu-rh.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5b054f-7b04-4c3e-944e-17aa2d9df1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1216977a-9588-481a-82c4-9bc34098f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#to read pdf\n",
    "from PyPDF2 import PdfReader\n",
    "#to make embeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "#to split text into chunks\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "#To create vectors and store them locally\n",
    "from langchain.vectorstores import FAISS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0a0b0-40a3-400d-9b5f-4a5f01fbc653",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# location of the pdf file/files. \n",
    "doc_reader = PdfReader('your pdf file name here')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba74cd6-a2a4-401b-a9ba-158105432e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc_reader\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9b5a14-06c5-4d71-b588-e4a84d122500",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read data from the file and put them into a variable called raw_text\n",
    "raw_text = ''\n",
    "for i, page in enumerate(doc_reader.pages):\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62abc091-0f7d-4d2a-a2be-90985875581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the length of the text\n",
    "len(raw_text)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f3c2fe-0b2f-4a8d-b3da-1b2eec363e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure it looks like something is there.\n",
    "raw_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7bdd83-f49a-4d51-b841-76fcc860f98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting up the text into smaller chunks for indexing\n",
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200, #striding over the text. We talked about this in class. We want an overlap\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d557e7ce-ffc5-44a7-bbc4-fd39f7859bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839057a-6b8b-4ade-8a7a-773621a130fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98edfe23-4618-4f71-ac68-f9669654deac",
   "metadata": {},
   "source": [
    "## Make the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51144a25-388c-40cd-bb28-9a7754187873",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download embeddings from OpenAI\n",
    "embeddings = OpenAIEmbeddings(openai_api_key= os.environ.get(\"OPEN_AI_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21105c84-0006-410b-b3b4-6af21e7552c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu\n",
    "#Faiss is a library for efficient similarity search and clustering of dense vectors.\n",
    "#It contains algorithms that search in sets of vectors of any size, up to ones that possibly \n",
    "#do not fit in RAM. It also contains supporting code for evaluation and parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bf406b-54a8-4d27-97fa-29489fc972a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "docsearch = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12763f7-2703-4e01-ac3c-50e7eedd9a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docsearch.embedding_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020b6585-cea0-40c4-9ce6-4034fbf57fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "query = \"how does GPT-4 change social media?\"\n",
    "docs = docsearch.similarity_search(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d33a0d0-4f9a-4d04-b280-b8abb8d78643",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b42f1e9-1eff-485b-860a-65f29ccedc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def95404-f177-43fd-8795-cc2c7fb58f82",
   "metadata": {},
   "source": [
    "## Plain QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6249684-93f6-49d3-9a2d-9a1f1cbe06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5214bb-0273-483e-ab41-8706da8de31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain = load_qa_chain(OpenAI(openai_api_key= os.environ.get(\"OPEN_AI_KEY\")), \n",
    "                      chain_type=\"stuff\") # we are going to stuff all the docs in at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49933983-3363-4edf-9571-49ce93df60e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check the prompt\n",
    "chain.llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def49fc6-9be5-41e2-bfeb-0731fdaba18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"who are the authors of the book?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5596e2c3-3863-4ff6-a65d-eb631c7f7bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the anwer correct? Why or why not?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2309567-bae0-4330-8cf9-323d4c786cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"who is the author of the book?\"\n",
    "query_02 = \"has it rained this week?\"\n",
    "docs = docsearch.similarity_search(query_02)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680a75a-a616-4cc1-b3f5-35a5e3859023",
   "metadata": {},
   "source": [
    "Why does this model return \"I don't know\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556aedd9-0964-4468-8286-c2e158378a35",
   "metadata": {},
   "source": [
    "## Your answer here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0704116d-f931-494e-9c31-a25b7d0342cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"who is the book authored by?\"\n",
    "#When you call docsearch.similarity_search(query, k=4), you are asking the function to perform a similarity search based\n",
    "#on the provided query and to return the top 4 documents that are most similar to the query. \n",
    "#This is commonly used in information retrieval systems, search engines, or document databases where the goal is to find a set of documents \n",
    "#that are most relevant or similar to a given query or document.\n",
    "docs = docsearch.similarity_search(query,k=1)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94183933-a615-47ec-ac23-d97f7817865a",
   "metadata": {},
   "source": [
    "# Will this return the correct answer? Experiment with different k values. Why are there so many different answers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579c1eec-14fe-493c-9c6d-716a4abe9df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d9fce0-a331-45ae-acaa-079716ff4e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa9e81b-f91b-4fbc-a889-b3ad51629c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Use the above code to answer this query = \n",
    "\"What is one of the biggest problems of AI?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4670aef-2da6-4299-bfd2-d7a1aff82c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed204f7-e50c-4912-a0db-5f8f5b829083",
   "metadata": {},
   "source": [
    "## WebUI for Small Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d7243-8380-40ee-a571-09f0132043dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Here is a video on install oogabooga. https://www.bing.com/videos/riverview/relatedvideo?q=installing+oobabooga&mid=2AF267039BE55F1AD6A52AF267039BE55F1AD6A5&FORM=VIRE\n",
    "Watch the video and install it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eaedcb-b7b5-4bd9-b8a4-43e5d991659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your confirmation that you've done it here: \n",
    "True or False?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b05f4af-96c3-4e5d-8344-71202ee32558",
   "metadata": {},
   "source": [
    "## Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd09ab16-a302-4f8c-8a5f-0d06806d5175",
   "metadata": {},
   "source": [
    "Provide some key bullet points and suggested data set and ask ChatGPT to generate a one page proposal for your final project in this class. Save the document to a text file. Then take your text file, load it into your ChatGPT web interface and ask it to create visualizations using matplotlib and seabotn of the ideas you express in your proposal. Take a screenshot of the image and insert it into your jupyter notebook. instructions here:\n",
    "https://stackoverflow.com/questions/32370281/how-to-embed-image-or-picture-in-jupyter-notebook-either-from-a-local-machine-o\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de9cd47-4338-4a45-a02a-e0bae7ba79ba",
   "metadata": {},
   "source": [
    "Your response here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f28338-a998-4a88-a247-4680d74cc19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Your image here:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
